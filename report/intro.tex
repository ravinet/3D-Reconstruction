\section{Introduction}
\label{s:intro}

Reconstructing a 3D object from multiple input images is an essential task in the field of computer vision. Recent advances in photo databases have led to an increase in the use of 3D reconstruction. While there have been many recent advances in 3D reconstruction, most new techniques still suffer from the same problems which plagued original methods. 

First, many approaches require a camera calibration for each image used in the 3D reconstruction. This calibration is necessary to obtain the intrinsic and extrinsic parameters of the camera which are used to project points from 2D images into 3D space. Performing calibrations for each image is impractical for new approaches that emphasize using as many images as possible for the reconstruction.

The other setback relates to user input. Many approaches require users to pick points on the input images that specify either the object to reconstruct or the planes used to perform the reconstruction. While user input may improve the reconstruction as it improves depth detection and crops the image so only necessary points are used, it does not encourage using many input images. Several techniques overcome these setbacks by performing reconstruction based on uncalibrated images obtained by the same, freely moving camera. Camera calibration is performed once and the resulting parameters are used for the reconstruction across all input images. 

Capturing pairs of feature points from the input images is an important step in the 3D reconstruction process. There are several feature point detection algorithms which are currently used by many reconstruction techniques, though each has its own setbacks. Two popular approaches are Scale Invariant Feature Transform (SIFT) and corner extraction algorithms such as Harris corner detection. Each approach yields pairs of matched feature points across the input images. 

In this project, we develop a new technique which reconstructs a 3D object from two input images captured by a freely moving camera. We combine SIFT and Harris corner detection to obtain the relevant pairs of feature points from the two input images. Prior to using these matched feature points, we use the Random Sample Consensus (RANSAC) algorithm to remove outlier matches. We calculate the intrinsic parameters of the camera used to take the input images with Zhangâ€™s method. Using the intrinsic parameters of the camera and the matched feature points from the input images, we are able to obtain a 3D reconstruction for the primary object in the image.

The report is organized as follows. Section~\ref{s:related} describes related work, and then section~\ref{s:overview} provides an overview of our algorithm. Then, the later sections delve into more details. Section~\ref{s:camera} and~\ref{s:sift} discuss camera calibration and automated feature detection which are tools that we use in our reconstruction. Section~\ref{s:fundamental} describes the relevant epipolar geometry, and section~\ref{s:reconstruction} explains how to use the epipolar geometry to do the 3D reconstruction. We show some results in section~\ref{s:results}, and we conclude in section~\ref{s:conclusion}.
